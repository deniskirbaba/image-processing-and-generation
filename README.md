# HW 4 - Distillation

## Task

В ходе домашнего задания попробуем разные виды дистилляции и их комбинации.

## Structure

```cmd
├── eda.ipynb
├── media/
├── README.md
├── src
│   ├── __init__.py
│   ├── metrics.py
│   ├── pipeline.py
│   └── utils.py
├── test-metrics-table.ipynb
├── distillation.ipynb
└── train.ipynb
```

## Dataset

В качестве датасета используется [CIFAR-10](https://paperswithcode.com/dataset/cifar-10).

> The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.

Решаемая задача - многоклассовая классификация.

### EDA

[EDA](./eda.ipynb) - минимальный EDA.

Для удобства используется уже реализованный в PyTorch класс для работы с CIFAR-10: `torchvision.datasets.CIFAR10`.

## Metrics calculation

Метрики:

* Precision - как по всем классам, так и усредненный по типу weighted, macro
* Recall - как по всем классам, так и усредненный по типу weighted, macro
* F1 score - как по всем классам, так и усредненный по типу macro

[Класс для расчета метрик с помощью `torch-ignite`](./src/metrics.py).

## Logging

Логгирование выполнялось с помощью `wandb`.

## Train, val, test pipeline

Функции для обучения, валидации, тестирования, дистилляции реализованы в [pipeline.py](./src/pipeline.py).

## Models

Модели учителя и студента будут иметь одну и ту же архитектуру `ResNet`.

* Учитель: `ResNet`, число слоёв 14, число параметров ~2.7M
* Студент: `ResNet`, число слоёв 8, число параметров ~1.2M

То есть студент имеет на ~50% меньше параметров чем учитель.

За основу взял готовую имплементацию модели ResNet18 [из PyTorch](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18).

## Train teacher, student on CIFAR-10

[Ноутбук с тренировкой моделей](./train.ipynb)

### Train/val/test results

![](./media/base-train/Screenshot%20from%202025-01-22%2014-36-09.png)
![](./media/base-train/Screenshot%20from%202025-01-22%2014-36-24.png)
![](./media/base-train/Screenshot%20from%202025-01-22%2014-36-32.png)
![](./media/base-train/Screenshot%20from%202025-01-22%2014-36-40.png)
![](./media/base-train/Screenshot%20from%202025-01-22%2014-36-45.png)

|         |   f1 avg |   precision macro |   recall macro |
|:--------|---------:|------------------:|---------------:|
| teacher | 0.751097 |          0.751185 |         0.7517 |
| student | 0.7189   |          0.738827 |         0.7163 |

## Distillation experiments

[Ноутбук с экспериментами по дистилляции](./distillation.ipynb)

## Experiment 1: Logits distillation

Используем уже обученную на прошлом этапе модель учителя с замороженными весами.

Параллельно будем подавать на вход учителю и ученику батч с изображениями и получать логиты и учителя и ученика.

Функция потерь будет состоять из 2-х компонент:

1. Hard loss: кросс-энтропия по ground truth тагретам и логитам ученика
2. Soft loss: KL-дивергенция по логитам ученика и учителя

### Train/val/test results (in comparison with standard student trained model)

![](./media/exp1/Screenshot%20from%202025-01-22%2017-01-16.png)
![](./media/exp1/Screenshot%20from%202025-01-22%2017-01-05.png)
![](./media/exp1/Screenshot%20from%202025-01-22%2017-01-28.png)
![](./media/exp1/Screenshot%20from%202025-01-22%2017-01-43.png)
![](./media/exp1/Screenshot%20from%202025-01-22%2017-01-48.png)

|                  |   f1 avg |   precision macro |   recall macro |
|:-----------------|---------:|------------------:|---------------:|
| distill (logits) | 0.737645 |          0.743072 |         0.7396 |
| student          | 0.7189   |          0.738827 |         0.7163 |
| teacher          | 0.751097 |          0.751185 |         0.7517 |

### Выводы

С помощью дистилляции по логитам учителя, нам удалось повысить все метрики в сравнении со стандартным обучением. При этом мы добились этого не увеличивая размер модели.

Данный подход работает вследствие того, что за счет добавления еще одной компоненты в функцию потерь, мы подаем модели сигналы не только о верном лейбле, но еще и о распределении вероятностей по этим лейблам. А так как в распределении вероятностей заложено намного больше смысла, чем в одном числе, модель эксплуатирует эти данные и лучше обучается.

## Experiment 2: Feature map distillation (untrained)

Также как и в прошлом эксперименте будет использовать предобученную модель учителя с замороженными весами.

В данном случае мы введем дополнительный компонент в функцию потерь студента, которая будет способствовать тому, чтобы выход скрытого слоя ученика стремился иметь такие же значения, что и на выходе более глубокого скрытого слоя учителя.

Таким образом, мы заставим ученика организовывать пространство на некотором скрытом слое так же, как это сделал уже обученный (более "умный") учитель.

Будем маппить 5 скрытый слой ученика на 9 скрытый слой учителя. Количество параметров в этом слое: `(128, 16, 16)`.

Так как размер слоёв один и тот же, то для сравнения слоёв нам не нужно использовать никакие операции по приведению размеров.

Здесь я специально выбрал слои одного размера и из середины обоих моделей.

Функция потерь будет состоять из 2-х компонент:

1. Hard loss: кросс-энтропия по ground truth тагретам и логитам ученика
2. Soft loss: MSE-ошибка отклонения значений весов в соответвующих выбранных слоях учителя и ученика

Для ускорения расчетов обрежем все слои у модели учителя, которые идут после выбранного слоя. Таком образом, мы не будем считать последующие слои.

### Результаты

![](./media/exp2/Screenshot%20from%202025-01-23%2020-06-41.png)
![](./media/exp2/Screenshot%20from%202025-01-23%2020-06-52.png)
![](./media/exp2/Screenshot%20from%202025-01-23%2020-07-15.png)
![](./media/exp2/Screenshot%20from%202025-01-23%2020-07-25.png)
![](./media/exp2/Screenshot%20from%202025-01-23%2020-07-31.png)

|                             |   f1 avg |   precision macro |   recall macro |
|:----------------------------|---------:|------------------:|---------------:|
| distill (hidden, untrained) | 0.714535 |          0.736665 |         0.7187 |
| student                     | 0.7189   |          0.738827 |         0.7163 |
| teacher                     | 0.751097 |          0.751185 |         0.7517 |

### Выводы

Эффективность данного типа дистилляции в данном случае не высокая. Метрики даже ниче чем простая тренировка без дистилляции.

Предполагаю, что это случилось, так как модель студента достаточно мала, чтобы выявить какие-то закономерности из скрытых слоёв учителя.

## Experiment 3: Feature map distillation (learning-base)

Эксперимент отличается от эксперимента 2 тем, что теперь мы добавим обучаемый слой перед тем как получить feature map ученика для последующего расчета компоненты дислилляции для функции потерь.

Так как выход является тензором размера `(128, 16, 16)`, то добавим один сверточный слой, который не будет изменять размер этого тензора. А именно `nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)`.

Расчет функции потерь точно такой же как в эксперименте 2.

### Результаты

![](./media/exp3/Screenshot%20from%202025-01-23%2020-47-46.png)
![](./media/exp3/Screenshot%20from%202025-01-23%2020-47-53.png)
![](./media/exp3/Screenshot%20from%202025-01-23%2020-48-02.png)
![](./media/exp3/Screenshot%20from%202025-01-23%2020-48-11.png)
![](./media/exp3/Screenshot%20from%202025-01-23%2020-48-16.png)

|                           |   f1 avg |   precision macro |   recall macro |
|:--------------------------|---------:|------------------:|---------------:|
| distill (hidden, trained) | 0.739198 |          0.754259 |         0.7374 |
| student                   | 0.7189   |          0.738827 |         0.7163 |
| teacher                   | 0.751097 |          0.751185 |         0.7517 |

### Выводы

Данный подход эффективнее, чем дистилляция без промежуточного обучаемого слоя (`f1` метрика выше на `0.2`). Наличие такого слоя обеспечивает то, что модель сама может скорректировать то, насколько сильно она будет обращать внимание на данный сигнал и увеличить вес одних или уменьшать других.

## Experiment 4: Logits distillation + feature map distillation (untrained)

Попробуем реализовать гибридную дистилляцию в которой будем "прокидывать" ученику как логиты учителя, так и активации со скрытого слоя (с обучаемой сверткой). Таким образом мы будем подавать ученику больше информации о том, как правильно организовать своё внутренне пространство + информацию о распределении классов на выходе. В таком случае обучение ученика должно пройти быстрее, так как сигналов обучения больше, а следовательно градиент будет немного выше, так как все эти сигналы складываются.

### Результаты

![](./media/exp4/Screenshot%20from%202025-01-24%2006-14-42.png)
![](./media/exp4/Screenshot%20from%202025-01-24%2006-14-58.png)
![](./media/exp4/Screenshot%20from%202025-01-24%2006-15-09.png)
![](./media/exp4/Screenshot%20from%202025-01-24%2006-15-21.png)
![](./media/exp4/Screenshot%20from%202025-01-24%2006-15-28.png)

|                  |   f1 avg |   precision macro |   recall macro |
|:-----------------|---------:|------------------:|---------------:|
| distill (hybrid) | 0.738803 |          0.74801  |         0.7403 |
| student          | 0.7189   |          0.738827 |         0.7163 |
| teacher          | 0.751097 |          0.751185 |         0.7517 |

### Выводы

Подход с гибридной дистилляцией тоже дал прирост в эффективности обучения студента. Кажется, что это наиболее универсальный способ, так как можно одновременно отдавать сигналы ученику как про верное распределение логитов с учителя, так и с выбранных внутренних слоёв учителя. Однако хоть такой способ и вносит гибкость, он также требует подбора дополнительных гиперпараметров - коэффициентов перед различными компонентами функции потерь. В данном случае их 2:

* `logits_dist_factor` - коэффициент определяющий важность уменьшения KL-дивергенции между распределением логитов учителя и ученика
* `hidden_trained_dist_factor` - коэффициент определяющий важность уменьшения MSE между выходами соответствующих внутренних слоёв учителя и ученика

## Общая сравнительная таблица

|                             |   f1 avg |   precision macro |   recall macro |
|:----------------------------|---------:|------------------:|---------------:|
| teacher                     | 0.751097 |          0.751185 |         0.7517 |
| distill (hidden, trained)   | 0.739198 |          0.754259 |         0.7374 |
| distill (hybrid)            | 0.738803 |          0.74801  |         0.7403 |
| distill (logits)            | 0.737645 |          0.743072 |         0.7396 |
| student                     | 0.7189   |          0.738827 |         0.7163 |
| distill (hidden, untrained) | 0.714535 |          0.736665 |         0.7187 |
